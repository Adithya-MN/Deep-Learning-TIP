{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Machine Learning\n",
    "\n",
    "What is Machine Learning? It is just a way to make a program learn using previous data points instead of being specifically hard-coded. Basically, machine learning is just curve-fitting, we use it to find relationships between multiple variables.\n",
    "\n",
    "For example:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 6]\n",
    "y = [3, 5, 7, 9, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we have a linear relationship between x and y. We have `y = 2x + 1`. Hence we can write it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 9, 13]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [(2*i + 1) for i in x]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this was a simple linear relationship with only a few data points, it was fairly easy to find the relationship between `x` and `y` merely by inspection. However it is very difficult or impossible to find out the relationship between variables when we have multiple variables, non-linear relationships or many datapoints.\n",
    "\n",
    "It is here that we make use of machine learning to make our program automatically learn the relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Broadly types of machine learning can be classified into two - Supervised and Unsupervised. In Supervised Learning, we already have a dataset of inputs and the corresponding outputs we expect to obtain from our inputs. Then we train a machine learning model on our given input-output pairs and which can then produce the corresponding output when a datapoint outside the training data is given\n",
    "\n",
    "For example if we use the `x` and `y` lists above as our training dataset, then our machine learning model should be able to give the correct answer for a value of `x` not in the training dataset after it has finished training. For example if we pass a value like `8` to our model, then it should output `17`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we determine if our model is training correctly? We use something known as a loss function. The loss function is basically a function to check how well our model is doing on the training examples. Our main goal during training is to minimize the loss function.\n",
    "\n",
    "In the beginning the loss will be very high since our model will be picking random values. But as the model trains more and more, and our loss function goes on decreasing, we can say that our machine is learning. There are many ways to decrease the loss function. The most popular among them is Gradient Descent, which is basically just finding the minima in the loss curve and moving towards it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two broad categories of Supervised Learning :-\n",
    "\n",
    "1. **Linear Regression** - This is the type of learning we did in the example above. Here we have to approximate a function which is a direct relationship between two variables. FOr example if we want to predict housing prices based on location, then we use Linear Regression on the location-housing price dataset, so that we can find the approximate housing price for a location not in the dataset\n",
    "\n",
    "\n",
    "\n",
    "2. **Logistic Regression** - This is the type of learning where instead of a specific value relationship, we are more concerned with the groups in which our dataset can be split into. For example if we had a dataset of pictures animals, then our model after training should be able to classify the pictures into the different types of animals. So that when we give our model a picture of a cow, it should output `Cow` as the tag of that picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspite of the many differences between Logistic and Linear Regression, at the basic level both these algorithms take in a labelled dataset, train on it via a loss function and an optimizer, and then make predictions based on what they have learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm\n",
    "\n",
    "Now we will use a library called `scikit-learn` to implement a very basic supervised machine learning algorithm known as K-Nearest Neighbors. It is the simplest supervised learning algorithm, since it doesnt use any loss function or optimizer like the examples above, and also doesn not have any specialized training phase.\n",
    "\n",
    "What this algorithm does is it use all the data points in the training set to classify any new instance or data that it gets. Whenever it gets a new data point, all this algorithm does is calculate the distance of this new data point with all the other points in the dataset, and then select the K nearest such points via distance.\n",
    "\n",
    "KNN is a classification algorithm. So after it finds the K nearest points, it checks to see the group into which the majority of these K points lie and then classify the new data-point as a member of that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how KNN works by using it on a simple dataset.\n",
    "\n",
    "First we have to import our dataset to be using with KNN. We will use the flower iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "\n",
    "dataset = pd.read_csv(url, names=names)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported the flower dataset using pandas, and stored the results in a `pd dataframe`. Let us check the head of the dataframe again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we have four indepented variables, which we will call attributes and one dependent varialbe, the class. Based on the values for our four attributes, we want to know which class a flower will be in.\n",
    "\n",
    "So first we have to split the dataset into the attributes and our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we have now stored the attributes in the numpy array `x` and the labels in the numpy array `y`.\n",
    "\n",
    "We are now ready to split the dataset into our training and test datasets. Since we have a single dataset, it is quite common in machine learning to split the dataset, so that we can prevent our model from being overly tuned to the training dataset. Hence we have a split so that we can train our model on the training dataset and then test it on our test dataset to see if our machine is actually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built in function in scikit learn we have split the dataset into `X_train` and `X_test`. The training dataset contains 80% of the data and the test dataset contains 20%.\n",
    "\n",
    "Now we have to normalize our data. Since our dataset may have attributes which are not in the same scale, this may lead to some attributes completely overshadowing other attributes just because of scale issues. Since that is undesirable, we have to normalize the data by subtracting the mean and divinding by variance. Doing this makes mean and variance of all our attributes 0 and 1 respectively. Thus scale issues vanish.\n",
    "\n",
    "In `scikit-learn` we can directly normalize by using the `StandardScaler` and its `transform` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. Since we are using `scikit-learn`, the inner workings of the algorithm are hidden from us and training our model is as simple as:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code makes a `classifier` object and then trains the classifier object on our training data. The `fit()` method aims to find a function that gives us the five nearest neighbors to a given data point with these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to test how good our model is. We do that as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       0.93      1.00      0.97        14\n",
      " Iris-virginica       1.00      0.80      0.89         5\n",
      "\n",
      "    avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tells us how good our model was. The precision is a measure of the ability of our model to not label an incorrect sample as correct. For example the precision for `Iris-setosa` is 1. This means that our model did not label any flower whihc wasn't `Iris-Setosa` as `Iris-Setosa`. \n",
    "\n",
    "The recall is a measure of how well our classifier was able to find all correct samples. FOr example the recall for `Iris-versicolor` is 1, this means that our model found all `Iris-versicolor` flowers. \n",
    "\n",
    "The f1-score is kind of like a weighted average of the precision and recall. THe support is the number of times each of thoses classes were present in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
